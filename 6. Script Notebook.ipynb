{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import datetime\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    #text = text.decode(\"UTF-8\")\n",
    "    text = text.replace('\\n',\" \")\n",
    "    text = text.replace('\\x0c',\" \")\n",
    "    text = re.sub(r\"-\", \" \", text) # Split the words with \"-\" (for example：pre-processing ==> pre processing）\n",
    "    text = re.sub(r\"\\d+/\\d+/\\d+\", \"\", text) # Take out the dates\n",
    "    text = re.sub(r\"[0-2]?[0-9]:[0-6][0-9]\", \"\", text) # Take out the time\n",
    "    text = re.sub(r\"[\\w]+@[\\.\\w]+\", \"\", text) # Take out the emails\n",
    "    text = re.sub(r\"/[a-zA-Z]*[:\\//\\]*[A-Za-z0-9\\-_]+\\.+[A-Za-z0-9\\.\\/%&=\\?\\-_]+/i\", \"\", text) # Take out the websites\n",
    "    pure_text = ''\n",
    "    # Validate to check if there are any non-text content \n",
    "    for letter in text:\n",
    "        # Keep only letters and spaces\n",
    "        if letter.isalpha() or letter==' ':\n",
    "            pure_text += letter\n",
    "    # Join the words are not stand-alone letters\n",
    "    text = ' '.join(word for word in pure_text.split() if len(word)>1)\n",
    "    return text\n",
    "\n",
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = spacy.load('en_core_web_sm')\n",
    "# Create our list of stopwords\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = parser(sentence)\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "    # return preprocessed list of tokens\n",
    "    return ' '.join(mytokens)\n",
    "\n",
    "def spacy_tokenizer_2(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = parser(sentence)\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens\n",
    "\n",
    "def n_topic_df(n):\n",
    "    lda_tfidf = LatentDirichletAllocation(n_components=n, random_state=0)\n",
    "    lda_tfidf.fit(word_matrix)\n",
    "    topic_matrix = lda_tfidf.transform(word_matrix)\n",
    "    topic_matrix_df = pd.DataFrame(topic_matrix).add_prefix('topic_')\n",
    "    topic_matrix_df[\"topic\"] = topic_matrix_df.iloc[:,:].idxmax(axis=1)\n",
    "    all_df = pd.concat([df_subID,topic_matrix_df], axis=1)\n",
    "    return all_df\n",
    "\n",
    "def n_topic_word(n):\n",
    "    lda_tfidf = LatentDirichletAllocation(n_components=n, random_state=0)\n",
    "    lda_tfidf.fit(word_matrix)\n",
    "    word_topic_matrix_df = pd.DataFrame(lda_tfidf.components_, columns=vocab).T.add_prefix('topic_')\n",
    "    return word_topic_matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/city_SanJose_Minutes.csv\")\n",
    "\n",
    "df = df.iloc[:, 1:]\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "art_df = pd.DataFrame(df.groupby('artID').sum('content')['subID'])\n",
    "\n",
    "art_df = art_df.loc[art_df.subID>3]\n",
    "\n",
    "df = df.merge(art_df, on='artID', how = 'inner')\n",
    "\n",
    "df_subID = df[df['subID_x']!=0]\n",
    "\n",
    "df_subID = df_subID.reset_index()\n",
    "\n",
    "text = df_subID['content']\n",
    "\n",
    "text = text.apply(lambda x: clean_text(x))\n",
    "\n",
    "text = text.apply(lambda x: spacy_tokenizer(x))\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=0.0085, max_df=0.9, stop_words=ENGLISH_STOP_WORDS)\n",
    "\n",
    "word_matrix = tfidf_vectorizer.fit_transform(text)\n",
    "\n",
    "vocab = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "all_df = n_topic_df(10)\n",
    "\n",
    "word_topic_matrix_df = n_topic_word(10)\n",
    "\n",
    "clean_list = [clean_text(i) for i in text]\n",
    "\n",
    "spacy_list =[spacy_tokenizer_2(i) for i in clean_list]\n",
    "\n",
    "w2v = gensim.models.Word2Vec(spacy_list, size=100, window=5, min_count=1, workers=2, sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Range and Key Words Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 2020-03-01\n"
     ]
    }
   ],
   "source": [
    "oldTime = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 2020-09-30\n"
     ]
    }
   ],
   "source": [
    "newTime = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " covid policy\n"
     ]
    }
   ],
   "source": [
    "keyWords = input(str())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyWords = keyWords.split()\n",
    "\n",
    "key_word=[]\n",
    "for i in range(20):\n",
    "    try:\n",
    "        key_word.append(w2v.wv.most_similar(keyWords ,topn=20)[i][0])\n",
    "    except:\n",
    "        print(\"Try Another Word\")\n",
    "\n",
    "topic_list =[]\n",
    "for i in key_word:\n",
    "    try:\n",
    "        topic_list.append(word_topic_matrix_df.loc[i].idxmax())\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "sub_df = all_df[(all_df['date'] > datetime.datetime.strptime(oldTime, '%Y-%m-%d')) & (all_df['date'] < datetime.datetime.strptime(newTime, '%Y-%m-%d'))]\n",
    "\n",
    "\n",
    "notes = pd.DataFrame()\n",
    "\n",
    "for j in range(len(list(set(topic_list)))): # number of unique topics\n",
    "\n",
    "    n = 0\n",
    "    for i in range(len(topic_list)):\n",
    "\n",
    "        if list(set(topic_list))[j] == topic_list[i]:\n",
    "            n = n+3  # Number of notes can be controled to show \n",
    "\n",
    "    notes = pd.concat([notes, sub_df.sort_values(list(set(topic_list))[j], ascending = False)[0:n]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220    10.2 20-219 GPT19-003/GP19-005/GP19-006 - Gene...\n",
       "355    2.19 20-463 Temporary Financial Relief to Non-...\n",
       "382    8.4 20-491 Substantial Amendments to the Fisca...\n",
       "405    10.2 20-481 SP19-064 - Special Use Permit to A...\n",
       "403    8.1 20-510 Actions Related to a Temporary Mora...\n",
       "467    3.4 20-576 Report on Digital Access and Inclus...\n",
       "472    7.1 20-562 Report on Digital Access and Inclus...\n",
       "469    3.6 20-582 Urgency Ordinance and Companion Ord...\n",
       "540    5.1 20-626 Actions Related to the Design and C...\n",
       "522    2.10 20-618 Proposed Extension of Natural Gas ...\n",
       "572    8.4 20-714 Economic Recovery Temporary Cap on ...\n",
       "621    3.9 20-757 COVID-19 Digital Inclusion Expendit...\n",
       "589    2.13 20-642 Annual Authorization for Workforce...\n",
       "627    8.1 20-717 Extension of Eviction Moratorium, R...\n",
       "615    3.3 20-696 Approval of Various Budget Actions ...\n",
       "726    8.7 20-781 Substantial Amendment to the FY 201...\n",
       "677    8.7 20-781 Substantial Amendment to the FY 201...\n",
       "767    8.1 20-888 Actions Related to the 2020-25 Cons...\n",
       "794    8.1 20-875 Downtown Residential High-Rise Incl...\n",
       "911    8.1 20-1094 Citywide Residential Anti-Displace...\n",
       "927    3.3 20-1123 Temporary Expansion of Business Ta...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes.sort_values('date').content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
